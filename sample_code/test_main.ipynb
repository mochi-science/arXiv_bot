{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\tashi\\\\Programming\\\\arXiv_bot'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するライブラリのインポート\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def fetch_papers_from_arxiv(search_query=None, max_results=3, csv_filename = \"datas/arxiv_papers.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    arXiv APIを使用して論文を取得し、CSVファイルに保存する関数\n",
    "\n",
    "    Parameters:\n",
    "        search_query (dict{\"thema\": \"query\"}): 検索クエリ\n",
    "            ex) queries = {\n",
    "                    \"LLM\": 'cat:cs.CL OR cat:cs.AI AND \"large language model\"',\n",
    "                    \"Machine Learning\": 'cat:cs.LG OR cat:stat.ML AND \"machine learning\"',\n",
    "                    \"XAI\": 'cat:cs.AI OR cat:cs.LG AND \"explainable AI\"'\n",
    "                }\n",
    "        max_results (int): 取得する論文の最大数\n",
    "        csv_filename (str): 保存するCSVファイル名\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: 新しい論文情報を含むDataFrame\n",
    "    \"\"\"\n",
    "    if search_query is None:\n",
    "        # テーマごとのクエリ\n",
    "        queries = {\n",
    "            \"LLM\": 'cat:cs.CL OR cat:cs.AI AND \"large language model\"',\n",
    "            \"Machine Learning\": 'cat:cs.LG OR cat:stat.ML AND \"machine learning\"',\n",
    "            \"XAI\": 'cat:cs.AI OR cat:cs.LG AND \"explainable AI\"'\n",
    "        }\n",
    "    \n",
    "    # 新しく取得した論文の情報を格納するリスト\n",
    "    new_papers_list = []\n",
    "\n",
    "    # 既存のCSVファイルを読み込む（存在すれば）\n",
    "    if os.path.exists(csv_filename):\n",
    "        existing_papers_df = pd.read_csv(csv_filename)\n",
    "    else:\n",
    "        # CSVが存在しない場合は空のDataFrameを作成\n",
    "        existing_papers_df = pd.DataFrame(columns=[\"Title\", \"Authors\", \"Published\", \"URL\", \"Abstract\", \"PDF\"])\n",
    "\n",
    "    # 既存のURL一覧を取得\n",
    "    existing_urls = existing_papers_df[\"URL\"].tolist()\n",
    "\n",
    "    # 各クエリで論文を取得し、既存のデータと比較\n",
    "    for theme, query in queries.items():\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,  # 各テーマで取得する最大論文数\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        for result in search.results():\n",
    "            # 論文情報を辞書にまとめる\n",
    "            paper_info = {\n",
    "                \"Title\": result.title,\n",
    "                \"Theme\": theme,\n",
    "                \"Authors\": ', '.join([author.name for author in result.authors]),\n",
    "                \"Published\": str(result.published),\n",
    "                \"URL\": result.entry_id,\n",
    "                \"Abstract\": result.summary,\n",
    "                \"PDF\": result.pdf_url\n",
    "            }\n",
    "\n",
    "            # 既存の論文リストにないか、または更新されている場合はリストに追加\n",
    "            if paper_info[\"URL\"] not in existing_urls or paper_info[\"Published\"] != existing_papers_df.loc[existing_papers_df[\"URL\"] == paper_info[\"URL\"], \"Published\"].values[0]:\n",
    "                new_papers_list.append(paper_info)\n",
    "\n",
    "    # 新しい論文があれば処理\n",
    "    if new_papers_list:\n",
    "        # 新しい論文のDataFrameを作成\n",
    "        new_papers_df = pd.DataFrame(new_papers_list)\n",
    "\n",
    "        # 既存の論文と新しい論文を結合\n",
    "        updated_papers_df = pd.concat([existing_papers_df, new_papers_df], ignore_index=True)\n",
    "\n",
    "        # CSVファイルに上書き保存\n",
    "        updated_papers_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        # print(f\"{len(new_papers_list)} 件の新しい論文を追加しました。\")\n",
    "\n",
    "    return new_papers_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..libs.fetch_arxiv import fetch_papers_from_arxiv\n",
    "# from libs.summarize import get_summary_by_gpt\n",
    "# from libs.to_message import send_message_to_discord\n",
    "\n",
    "# def main():\n",
    "#     # arXiv APIを使用して論文を取得し、CSVファイルに保存\n",
    "#     df_papers = fetch_papers_from_arxiv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tashi\\AppData\\Local\\Temp\\ipykernel_29716\\76235368.py:52: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "df_papers = fetch_papers_from_arxiv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "def get_summary_by_gpt(api_key: str, df_papers: pd.DataFrame) -> dict:\n",
    "    \"\"\"論文の内容を要約したものを返す\n",
    "\n",
    "    Args:\n",
    "        api_key (str): OpenAI API キー\n",
    "        df_papers (pd.DataFrame): 論文データ\n",
    "\n",
    "    Returns:\n",
    "        dict: レスポンスデータ（jsonをdictに変換したもの）\n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # リクエストのプロンプトを作成する\n",
    "    prompt = \"論文の概要を日本語でそれぞれ要約して。要約文のみ出力。\"\n",
    "    prompt += \"出力例：pythonのlist形式。\"\n",
    "    prompt += str(df_papers[\"Abstract\"][:2].to_list())\n",
    "\n",
    "    # OpenAI API にリクエストを送信する\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"読者の興味を引く要素を強調\"}, # 何かを知るにはまず興味をそそられることが重要\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.to_dict()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "key = json.load(open(\"openai_apikey.json\"))[\"KEY\"]\n",
    "\n",
    "res = get_summary_by_gpt(api_key=key, df_papers=df_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここにSlackなどのメッセージ送信用の関数を記述する\n",
    "\n",
    "from discord_webhook import DiscordWebhook\n",
    "\n",
    "def send_message_to_discord(message: str, webhook_url: str):\n",
    "    \"\"\"Discordにメッセージを送信する\n",
    "\n",
    "    Args:\n",
    "        message (str): 送信するメッセージ\n",
    "        webhook_url (str): DiscordのWebhook URL\n",
    "\n",
    "    Returns:\n",
    "        response: webhookによるレスポンス\n",
    "    \"\"\"\n",
    "    # メッセージを送信\n",
    "    webhook = DiscordWebhook(url=webhook_url, content=message)\n",
    "    response = webhook.execute()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chain-of-thought (CoT)プロンプトは、大規模言語モデル(LLM)から推論能力を引き出すための実質的な方法ですが、どのようなタスクでこの「思考」が本当に有益なのかを分析するために、100以上の論文を対象とした定量的メタ分析を実施し、14モデルで20のデータセットを評価しました。その結果、CoTは数学や論理に関するタスクで強い性能向上を提供し、他のタスクでは効果が小さいことがわかりました。また、MMLUでは、CoTを使用しなくてもほぼ同じ精度が得られることが示されました。この結果を受けて、計画と実行を分けてCoTの挙動を分析し、ツール補強されたLLMと比較しました。CoTの多くの利点は象徴的な実行の改善から来ており、象徴的ソルバーを使用することと比較すると劣ります。CoTは選択的に適用され、推論コストを節約しつつ性能を維持できることを示唆しています。',\n",
       " '個別化されたアウトフィット推薦は、ファッションの互換性とトレンドの理解を必要とする複雑な課題です。本論文では、大規模言語モデル（LLM）の表現力を活かした新しいフレームワークを提案し、LLMの「ブラックボックス」性と静的な特性をファインチューニングと直接フィードバックの統合によって緩和します。視覚テキストのギャップを埋めるために、マルチモーダル大規模言語モデル（MLLM）を用いた画像キャプショニングを活用し、ファッション画像からスタイルやカラーの特徴を抽出します。LLMはオープンソースのPolyvoreデータセットで効率的にファインチューニングされ、スタイリッシュなアウトフィットを推奨する能力を最適化します。ネガティブ例を使用した直接的な嗜好メカニズムが、LLMの意思決定プロセスを向上させるために採用され、季節のファショントレンドに合致したアウトフィット提案を継続的に洗練する自己強化型AIフィードバックループが形成されます。このフレームワークはPolyvoreデータセットで評価され、空白埋めや補完アイテム探索の2つの主要タスクでその効果が示されました。評価結果により、提案したフレームワークがベースLLMを大幅に上回り、より統一感のあるアウトフィットを生成できることが示されました。このタスクにおける性能向上は、正確な提案でショッピング体験を向上させるポテンシャルを強調しています。']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "webhookurl = json.load(open(\"sample_code\\webhook_url.json\"))[\"URL\"]\n",
    "papers_summary = literal_eval(res[\"content\"])\n",
    "\n",
    "# send_message_to_discord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
